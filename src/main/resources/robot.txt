1. Why REST services should be stateless?
Because it enables the fault tolerance and scalability: requests from the same client can be processed on different servers.

2. What requests are cacheable and how can they be implemented?
GET requests are cacheable. It can be defined by adding an HTTP header Cache-Control

3. Why events can be used for pluggable architecture?
Events provide a possibility to attach additional functionality to any part of the application.
Event listeners make possible to avoid changing the core part of application.

4. How to make event listeners to work on separate threads?
By default, the event listeners are working on the main thread.
You can define bean ApplicationEventMulticaster in @Configuration to allow to use multiple threads.
It allows to run several event listeners in parallel and takes the threads from the thread pool, making threads reusable.


1. What are the use cases for using ETag?
ETag is used when we retrieve the same data and need to know if it was changed. 
If it was not, we will get HTTP status 304 (Not Modified).

2. Why does an ETag value represent a hash value of the object?
That's because the data can be retrieved in many ways, but if the hash has not changed, 
we suppose that data are not changed as well.

3. Why does using ETag may increase the application performance?
Because the server do not pass the whole data to the client if it was not changed 
(and therefore server has to transfer less data)

4. How can we use ETags in a Spring application?
We need to add ShallowEtagHeaderFilter to generate ETags on the server-side and perform GET requests 
on the client-side with HTTP header If-None-Match: ETag.


1. Can you provide some examples of when REST events are useful?
In Spring Data REST, REST events can define additional behaviour for processing the entity, for example, 
it can be used for validation of entity before it is saved (event handler should throw an exception if 
validation is failed, and it will prevent data modification in the database).

2. Is it possible to use REST events with a REST controller?
No, REST events can be used only in Spring Data REST, with auto-generated REST controller
, and allows to define some behaviour in addition to the default.

3. Why should we use HATEOAS? What are the use cases?
1) HATEOAS allows us to provide auto-generated documentation of the REST service: we get not just data, but also some "where to go" links, like on the web page.
2) HATEOAS gives a higher level of abstraction: we are able not to rely on the exact hard-coded URLs, but instead, it can be auto-generated (for example, point to the different servers).
3) HATEOAS can provide some standard links like "self" to get the actual data (re-read), or "details" to get more detailed data.
4) Some JSON data can be replaced by the link to the additional data, for example, we can omit the complete address of the person, and add link "/address" to get the detailed information if necessary.

4. What do we need to add HATEOAS links?
We need to implement a RepresentationModelAssembler which includes 2 methods: toModel() which converts entity to EntityModel and adds a link to the single entity, and toCollectionModel() which converts a list of entities to the Collection of EntityModel with links attached to the whole collection.

5. Why RestTemplate is about to be deprecated? Should we avoid it?
It is outdated because it doesn't support reactive programming, and the Spring team recommends using the more universal and contemporary WebClient. However, many projects are still using RestTemplate and most probably it won't be deprecated soon.

1. Why do we need to call method block() in WebClient?
Since WebClient has a reactive interface, we need to call block() to wait until the data will be retrieved. Otherwise, we'd get the unblocking behaviour and will need the reactive operators to process it.

2. What is BodyExtractors in WebClient, and when should we use it?
BodyExtractors allows deserializing the received data (typically JSON). 
We should use it to read and process the response from the server.

3. Why do we need methods BodyExtractors.toMono() and BodyExtractors.toFlux()? What parameter do they take?
These methods are needed to read a server response and to convert a single JSON object with BodyExtractors.toMono() or JSON array with BodyExtractors.toFlux() 
to a Java object or collection. As a parameter, it takes the class of the data which we should retrieve, 
like Person.class: BodyExtractors.toMono(Person.class) will deserialize a JSON to an object of class Mono<Person>

4. Why is it recommended to use Open Feign?
Open Feign is working much faster than RestTemplate and needs much less code. It allows to define only 
the interface of the REST client for efficient use, the same way as it is done in Spring Data.


1. What is Open API and which versions of Open API can be used?
Open API is a standard for documenting REST API. It can be represented as YAML or JSON. There are 
2 widely used versions of Open API: 2 and 3, and Swagger supports both.

2. What is Swagger and what tools does it provide?
Swagger is the most popular set of tools to work with Open API. Swagger UI is used to visually represent
Open API to make it convenient for the end-users (usually frontend developers). 
Swagger Editor is used to create and edit Open API documents (it is represented as YAML). 
Swagger Codegen allows generating a server and a client source code based on Open API specification.

3. What is SpringFox and when should we use it?
SpringFox is a Java library that allows to automatically generate Open API specifications from the source code of Spring REST services. 
Then it can be represented with Swagger UI.

4. What is the difference between Design First and Code First approach? Which tools can be used to implement these approaches?
Design First means that first of all analytics are creating REST API specifications, and only then the server is implemented. 
Code First means that the server is created first, and then the REST API specifications are generated from the server. 
For Design First, we can use Swagger Editor to create specifications + Swagger Codegen to generate the code. 
For Code First, we can use SpringFox to generate the documentation from the server.

5. How can we configure SpringFox to customize the REST documentation?
We can use @Api and @ApiOperation annotations to rename or describe the REST operations as it is described in Open API 
specification. Also, we can filter out some endpoints by their URLs or by the HTTP method if we don't want them
 to be present in the external API.
Also, we can define meta-information like authors, license, etc.



@@@ Module 2. Object mapping

### Limiting data passed to the client: basic approaches(DTO, Nullifying, JsonIgnore)

1. Why do we need to limit data passed to the client-side?
It is necessary because otherwise, we may pass too much data to the client (in the worst case the whole tree of objects will be serialized to JSON and passed to the client). Also, some data like passwords should be always omitted.

2. What is the DTO pattern and why should we use it?
DTO is Data Transfer Object - the most powerful way to prepare data that should be passed to the client. 
However, it needs a lot of extra work to perform converting.

3. What is nullifying fields approach?
Nullifying fields means that we set some properties of the object which will be serialized to JSON to null.
Typically, it is performed in the REST controller and doesn't change the database - we just exclude some data from serialization.

4. What Jackson annotation can be used to limit the data passed to the client and what is a drawback of this approach?
Jackson has the annotation @JsonIgnore which allows excluding some property from serialization. The drawback is that @JsonIgnore is not flexible, i.e. it excludes the property forever.
Other approaches, like nullifying fields and DTO are more flexible.


### Limiting data passed to the client: special tools

- What are projections, how to apply them and when can they be used?
Projections and excerpts are working [only in Spring Data REST] and can be used with Spring Data repositories.
They allow creating subsets of data to be serialized. This is achieved by adding projection parameters 
[to the URL] with the name defined with the @Projection annotation.

- What are the excerpts, how to apply them and when can they be used?
Excerpts are like a default projection for the Spring Data repository.
However, it is applied [!only] to the collections, [!not] to single entities.
For a [!single entity], we have to use the projection parameter.

- When and why should we use MapStruct? What is the advantage of using MapStruct?
MapStruct automates the generation of DTO for an entity.
The advantage of MapStruct is that it allows [[avoiding]] the boilerplate code 
and defining [only non-trivial] mappings of Entity to DTO and DTO to Entity.

- How can a Jackson serializer be used to customize the serialization to JSON? What is the benefit of using it?
A Jackson serializer is a [low-level] approach to customize serialization.
We can use @JsonSerialize and @JsonDeserialize to apply the custom serializer.
The benefit is that we can [avoid adding] a DTO layer, and we can provide some common rules for serialization 
(for example, the field with the name "password" shouldn't be serialized).


### Java Bean Validation framework
1. What are the advantages of using the Java Bean Validation framework?
1) It is a JEE standard (Spring uses Hibernate Validator as implementation)
2) It provides a declarative approach, which makes the code easier to read, 
because data constraints are defined with annotations
3) It is customizable, we can create our own validators
4) Standard validators can be propagated to database constraints

2. How to create a custom validator for the Java Bean Validation framework?
1) Create the annotation with the name of validator
2) Create a class implementing ConstraintValidator having the method boolean isValid()

3. How to use an entity validator in a REST controller?
1) Add the annotation @Valid to @RequestBody
2) Provide a handler for MethodArgumentNotValidException, returning JSON if the validation is failed
3) Put a handler to the REST controller which extends ResponseEntityExceptionHandler and is marked 
as @ContollerAdvice - this way you will make it to handle all exceptions of this kind

4. How to use a validator with request parameters?
1) Add validation annotations to the controller method parameters, like @PathVariable @Min(1) Long id
2) Add the annotation @Validated to the REST controller class
3) Provide a handler for ConstraintViolationException using method marked with @ExceptionHandler
4) Put the handler to the REST controller marked as @ContollerAdvice - this way you will make it handle all exceptions of this kind

@@@ Module 3. Transactions in Spring

### Transactions in Spring
1. What are the ACID properties of transactions?
Atomicity means that either all operations in a transaction are committed, or all are rolled back.
Consistency means that the database is in a consistent state after the transaction commits or rolls back.
Isolation means that the transaction doesn't interact with the effects of other transactions.
Durability means that if the transaction is committed, the changes cannot disappear 
(this is achieved by first writing to the transactional log, which is applied again in case of any failure).

2. What is a PlatformTransactionManager? What are the possible implementations of this manager?
PlatformTransactionManager is a central interface for working with transactions.
It is used in declarative transaction management implicitly and can be used for programmatic transaction management by injecting its instance.
It has only 3 methods: getTransaction(), commit() and rollback().
There are various implementations of this interface, and the most frequently used are DataSourceTransactionManager for plain JDBC, JpaTransactionManager to be used with JPA, HibernateTransactionManager for Hibernate, and JmsTransactionManager for working with message brokers.

3. What is a TransactionDefinition? What parameters does it have?
TransactionDefinition is an interface used to configure a transactional method.
TransactionDefinition defines 4 properties: isolation, propagation, timeout, and read-only status.

4. Why should we set a timeout for a transaction? What is the default timeout?
We should set the timeout to avoid too long transactions.
The unit of timeout is in milliseconds.
If the request is not completed during this time, we'll get TransactionException (transaction time expired).
Setting timeout might be useful for rolling back long-running database queries.
The default value is -1 (no limit).

5. Why should we set readonly to true?
This attribute is the hint for the JPA provider (Hibernate) and JDBC driver 
that we are not going to write to DB, which allows performing some additional optimizations.
The result is that insert/update queries will not be executed.
Also, if you read data and update data, these updates will not be persisted on commit.

6. Why do we need a transaction for reading data?
1) Results consistency: when we want to run more than one SELECT, 
and we want the results to be consistent, we should do it in a read-only transaction.
2) If we want to be sure that no attempt to write data to DB will happen.

### Transaction isolation levels

- What is the READ UNCOMMITTED isolation level? 
READ UNCOMMITTED imposes a [[!minimal|maximal]] number of locks. 
It [[!prevents only|doesn't prevent]] lost updates - when data are changed by one transaction, the same data [[!cannot be|can be]] changed by another transaction until commit or rollback. 
In the same time, [[!it may|it doesn't]] allow dirty reads, unrepeatable reads, and phantom reads.

- When should you use READ UNCOMMITTED isolation level?
It can be used if we [[!need|do not need]] a maximal performance and may consider [[!acceptable|not acceptable]] the dirty reads and unrepeatable reads.
* If we are running aggregations on many rows: for example, the [[maximum|!average]] of one million values will not change significantly
* If we are reading [[global|!local]] data, that no one else is modifying
* If we are reading the [[!old|new]] data, that no one is supposed to modify anymore

- What is the READ COMMITTED isolation level?
This level prevents reading not committed data ([[!dirty reads|unrepeatable reads]]). 
If we update something in the transaction, this data [[!cannot|can]] be read by other transactions. 
It is achieved by acquiring a lock when we [[read|!change]] data. 

- When should you use the READ COMMITTED isolation level?
It is the default isolation level in many databases (Oracle, PostgreSQL, etc.). 
It provides a good balance between [[consistency|!performance]] and isolation. 
Unrepeatable or phantom reads [[!may|may not]] occur on this level.

- What is the REPEATABLE READ isolation level?

It is an isolation level that prevents dirty reads and [[phantom|!unrepeatable]] reads. 
If a transaction is [[changing|!reading]] some data, this data cannot be changed by other transactions. 
It is achieved by acquiring a lock when we are [[updating|!reading]] data. 
The [[benefit|!drawback]] is that we get [[not so much|!a lot of]] locks: we lock everything we look at. 

- What is the SERIALIZABLE isolation level?

SERIALIZABLE is a level that provides [[partial|!complete]] isolation. 
It [[doesn't prevent|!prevents]] all possible problems, including phantom reads. 
However, it comes at a cost of too much locking, which makes the database work [[faster|!slower]].

### Problems solved by transaction isolation

- What is a lost update problem?

The lost update problem is a situation when a database is changed by multiple users and the changes of one user are lost. 
For example, if the first user is reading some data, for example A=1, and then the second user is [[!also reading|changing]] the same value, A=1. 
Then the first user sets A=A+1 and saves its value 2 to the database. 
The second user does the same: calculates A=A+1 [[but do not save|!and saves]] value 2 to the database. 
As a result, we had 2 increments: +1 by the first user and +1 by the second user. 
However, the database value is [[!still 2|still 1|already 3]], which means that the second user overwrites changes made by the first user. 
We have got a lost update problem: changes of the [[!first|second]] user are lost.

- What is a dirty read problem?

Dirty read is reading [[!not consistent|consistent]] data or data from the [[end|!middle]] of work of some transaction (dirty data). 
It happens when one transaction is changing data [[and|!but does not]] commit it, and another transaction is reading data that is [[already committed|!not committed yet]]. 
It can be solved by preventing the reading of any changed data (acquiring a lock from reading). 
It is solved using the [[!READ COMMITTED|READ UNCOMMITTED]] isolation level.

- What is an unrepeatable read problem?

Unrepeatable read is a problem when we read data in the transaction, then after some time read the same data in [[!the same|another]] transaction - and get [[same|!different]] results. 
It can be resolved by [[allowing to|!preventing the]] change of the data which was read (by [[releasing|!acquiring]] a lock when we read data). 
It is solved using the [[!REPEATABLE READ|READ COMMITTED]] isolation level. 

- What is a phantom read problem?

Phantom read is a problem when we perform several same SELECT ... FROM ... queries in the transaction [[and get the same|!but get different]] results. 
It happens because other transactions [[!may|may not]] add new rows (phantoms), and query results may change. 
It is solved at the [[READ COMMITTED|!SERIALIZABLE]] isolation level by acquiring a lock for [[!the whole table|all data we read]].

- When should you use the SERIALIZABLE isolation level?

If we want to be sure that we have complete isolation of transactions, we should use the [[READ COMMITTED|!SERIALIZABLE]] isolation level. 
Using this level will make the database work [[faster because of few|!slower because of many]] locks.

### Transaction propagation

- What is a transaction propagation?

Transaction propagation defines what will happen when one [[transaction|!transactional method]] is calling another one. 
Should it reuse the same transaction, or start a new transaction, or prevent running in a transaction, or require a transaction?

- What is a Required transaction propagation and when should it be used?

Required is a [[frequently used|!default]] transaction propagation for @Transactional annotation. 
It should be used when [[!we|we do not]] need a transaction for the method (we always need it if we [[!change any|read]] data), 
but we want to be able to call the method [[either|!both]] from a transactional method or from a non-transactional method.

- What is RequiresNew transaction propagation?

A method marked as RequiresNew means that it [[may need|!always needs]] a new transaction. 
If the method is called from a non-transactional method, it [[uses the same|!starts a new]] transaction. 
If a method is called from a transactional method, it [[rollbacks|!suspends|commits]] a running transaction and creates a new transaction. 

- When should you use RequiresNew transaction propagation?

The behavior of RequiresNew violates the [[isolation|!atomic]] principle: it may happen that part of the transaction will roll back, while the transaction will commit, or vice versa. 
If transactional methodA() is calling methodB() marked as RequiresNew, methodB() may roll back when methodA() [[roll back|!commits]], or methodB() may [[!commit|rolled back]] when methodA() is rolled back. 
Therefore, this propagation can be used only for some separate part of work, [[related|!not related]] to the outer transaction. 
For example, it can be used for writing log messages (it [[should not|!may]] commit if a transaction will roll back), or for caching, or for synchronization - for something where the failure of the part [[should|!may not]] lead to the failure of the whole.

- What is Mandatory transaction propagation and when it should be used?

Mandatory transaction propagation means that a method can be called only from the transactional method [[!but doesn't|and will]] start the transaction. 
It may be used if the method needs a transaction, but it [[should|!shouldn't]] be used independently, only as a part of some multi-step algorithm. 
In case if the method marked as Mandatory [[!is not running|is running]] within a transaction, TransactionRequiredException is thrown.

- What is NotSupported transaction propagation and when it should be used?

NotSupported means that the method is not supporting transactions. 
It should be run from [[only transactional|!non-transactional]] method, or if it is run from a transactional method, the transaction will be [[!suspended|rolled back]]. 
It can be used [[for updating|!only for reading]] data. 
Because transactions involve overhead, this attribute may improve performance (for example, avoid acquiring locks when [[updating|!reading]] data). 

- What is Supports transaction propagation and when it should be used?

Supports means that method is not changing the behavior of a caller method: if the caller method was transactional, it will be executed [[without|!within]] a transaction, otherwise, it will be executed [[within|!without]] a transaction. 
It can be used [[for updating|!only for reading]] data, and most often in conjunction with readOnly=[[!true|false]]. 
In this case, we avoid the performance loss associated with creating a transaction and suspending a transaction, because all we need here is to read the data.

- What is Never transaction propagation and when it should be used?

Never means that a method should never be called from a [[non-transactional|!transactional]] method. 
If it was, we'd get IllegalTransactionStateException.
It can be used for [[short|!long-running]] methods which should [[!never be|be]] executed inside a transaction because it may lead to low performance of database (locks will be held for a long time), and either [[!not need|need]] transactions or use inner [[declarative|!programmatic]] transactions. 
It may also be used for testing purposes, to verify that a method is not transactional. 

### Exceptions and transactions rollback

- How to rollback a transaction when using declarative transactions?

By default, any [[Exception|!RuntimeException|RollbackException]] leads the transaction to be rolled back. 
Otherwise, it's possible to change a list of exceptions that may lead to rollback using the [[!rollbackFor|rollbackOnly]] property of the @Transactional annotation.

- What is the difference between logical and physical transactions? 

[[!Physical|Logical]] transactions are actual database transactions, while [[physical|!logical]] transactions are @Transactional-annotated Spring methods. 
Typically, a single [[logical|!physical]] transaction may include several [[physical|!logical]] transactions.

- What happens with a physical transaction if the logical transaction is throwing an exception?

If a @Transactional method throws an exception that is listed in rollbackFor (or any RuntimeException by default), the outer physical transaction will [[!not commit|rollback]] and will throw [[!UnexpectedRollbackException|RollbackOnlyException]]. 
It will happen because throwing one of the exceptions from the rollbackFor list will mark the physical transaction as [[committed|!rollback-only]], so it will not be able to [[rollback|!commit]] even if the exception was caught. 

- Is it possible to call a transactional method from the same class?

[[Yes|!No]], this method will [[!not be|be]] transactional in this case, because transactions in Spring are using the proxy mechanism, which can [[!only|not]] be used only if Spring injects a reference to a proxied [[!implementation|interface]], which adds transactional behavior. 
You can use the [[!self-injection|auto-injection]] pattern if you need to call a transactional method from the same class.

### Optimistic and pessimistic locking

- In which scenario optimistic or pessimistic locking should be used instead of transactions?

Transactions are used for a [[long|!short]]-running set of operations. 
If we need some [[short|!long]]-running operations, especially if a human is involved, we cannot do them in a transaction, because we shouldn't hold a connection and the locks for a long time. 
We should use the approach named application transaction or long conversation. 
A typical scenario involves a user who needs to change some data in the database: the user opens data in a form (reads data), then edits data in the form, then pushes the Save button to write the updated data back to the database. 
In this scenario, we may face a [[dirty read|!lost update|unrepeatable read]] problem. 
For example, the user Alice and the user Bob are opening the same data, then the user Alice saves her changes, then the user Bob saves his changes. 
In this scenario, [[Alice|!Bob's]] changes will overwrite the changes made by [[Bob|!Alice]]. 
How to solve it? We may forbid to change data that was opened by someone else (it can be opened in read-only mode) - this option is named [[optimistic|!pessimistic]] locking. 
Otherwise, we may allow everyone to change the data, but only changes made by the first user will be successfully saved - it is named [[!optimistic|pessimistic]] locking (because we hope that the first user will be the only one). 
Typically, [[!optimistic|pessimistic]] locking is preferable for this scenario.

- What is optimistic locking and how does it work?

Optimistic locking is a strategy to prevent concurrent updates of the same data by multiple users [[by|!without]] using locks. 
To make it possible, the data entity has a special property (of type int/long/Date) marked with the [[!@Version|@Lock]] annotation. 
Usually, it is a number initially set to 0. When the entity is read, the version is also read. 
If anyone changes the data, [[we should increase|!it automatically increases]] a version. 
If we need to save the updated data, we should first check if the version has been changed since we read the data (it is done automatically by Hibernate). 
If it has [[!not been|been]] changed, we can be sure that nobody else touched the data (represented as a database row) and we are working with this data exclusively. 
Otherwise, if the version is [[the same|!different]], it means that the data has been changed by somebody else, and we will get [[!OptimisticLockException|WrongVersionException]] when trying to save this data. 
In this case, we should roll back our change and re-read data from the database to get the actual data. 
Then we can show it to the user or merge our changes with the updates from the database and try to save it again. 
This strategy allows avoiding [[locks|!lost updates]] because changes of the same data made by another user will not be overwritten by us.

- What is pessimistic locking and how does it work?

Pessimistic locking is called "pessimistic" because the system assumes the worst â€” that two or more users will want to update the same record at the same time, and therefore the system prevents that possibility by locking the record, no matter how unlikely conflicts are. 
Pessimistic locking is used internally and automatically when we use transactions (depending on the isolation level). 
However, it is possible to [[automatically|!manually]] acquire the lock for the entity by using entityManager.lock(entity, PESSIMISTIC_WRITE), or the @Lock(PESSIMISTIC_WRITE) annotation for the method - in this case, we may acquire the lock at the moment of reading data. 
It will execute a [[LOCK DATA|!SELECT FOR UPDATE]] query, which will acquire the [[shared|!exclusive]] lock. 
The lock will prevent changing data (because changing data needs to acquire an exclusive lock) and prevent acquiring the lock for this data by other transactions. 
Otherwise, we can acquire PESSIMISTIC_READ (shared) lock which wouldn't allow changing data but will assure that anyone else wouldn't be able to get an exclusive lock to change the data. 
If the database does not support shared locks (for example Oracle), then a shared lock request (PESSIMISTIC_READ) will simply acquire an exclusive lock request (PESSIMISTIC_WRITE). 
Pessimistic locks are automatically released at the transaction end (on commit/rollback) and should [[not be used|!be used only]] inside the transaction.

### Distributed transactions

- What is a distributed transaction and how does it work?

A distributed transaction is a set of operations on the data that is performed across [[one database|!two or more databases]] (running on [[the same|!different]] servers). 
It can be implemented by using a [[three-phase|!two-phase|one-phase]] commit scenario: 
1) The transaction dispatcher starts the distributed transaction, and all the participants must vote that they have acquired all necessary [[data|!locks]] and are ready to commit the transaction - this state is named [[!transaction prepared|database ready]]. 
2) If any participant of the distributed transaction couldn't [[commit|!prepare]] the transaction (wasn't able to acquire the necessary locks), the transaction is marked for rollback and all participants should rollback it.
3) If all participants were able to prepare the transaction, the transaction dispatcher says that distributed transaction [[cannot be|!can be]] fixed. Then, the commit is executed for all participants of the distributed transaction.

- How can a distributed transaction be used in Spring?

Spring supports distributed transactions through JTA (Java Transaction API, part of JEE specifications), or by using the ChainedTransactionManager class built into Spring. 
However, ChainedTransactionManager is not a reliable solution and has been deprecated. 
You can use JTA with some JEE application server or a lightweight JTA provider like Atomikos/Bitronix/Narayana ([[but only if you|!if you don't want to]] use a JEE application server). 
Otherwise, you can use the [[optimistic lock|!SAGA pattern]].

- What is the SAGA pattern and when it should be used? 

SAGA is the best option to implement distributed transactions, which may be used not just for relational databases, but also for NoSQL databases and various other data sources. 
SAGA needs some messaging channel to exchange data between servers (it can be Kafka/Rabbit/JMS). 
Every transactional operation A in SAGA should have its reverse counterpart (to perform the rollback of the operation) - operation A'. 
For example, if we put some money into the account (operation A), we should define the reverse operation to [[deposit|!withdraw]] the same amount from the account (operation A'), which will [[commit|!rollback]] the first operation. 
Let's assume that we have 2 services involved in a distributed transaction, and we need the first service to execute operation A, and the second service to execute operation B. 
After operation A is successfully completed, the first service is sending a message to the second service to run operation B. 
If operation B completes successfully, the service sends an approval message to the first service about the successful completion. 
If operation B fails, the second service is sending a message to the first service to [[commit|!rollback]] operation A, and the first service performs operation A' which will [[commit|!rollback]] operation A. 
The same will happen if the first service doesn't get [[a response|!an approval message]] from the second service for a long time (for example, if the second service is down).
`
